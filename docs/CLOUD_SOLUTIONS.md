# Soluciones Cloud para PySpark y Airflow

Gu√≠a completa de opciones para ejecutar PySpark y Airflow sin necesidad de infraestructura local.

---

## üìä Comparaci√≥n R√°pida

| Soluci√≥n | Costo | PySpark | Airflow | Integraci√≥n Web | Dificultad |
|----------|-------|---------|---------|-----------------|------------|
| **Google Colab** | üÜì Gratis | ‚úÖ S√≠ | ‚ö†Ô∏è Limitado | ‚≠ê‚≠ê‚≠ê F√°cil | F√°cil |
| **Databricks Community** | üÜì Gratis | ‚úÖ S√≠ | ‚ùå No | ‚≠ê‚≠ê Media | Media |
| **GCP Free Tier** | üí∞ $300 cr√©ditos | ‚úÖ S√≠ | ‚úÖ S√≠ | ‚≠ê‚≠ê‚≠ê F√°cil | Media |
| **AWS Free Tier** | üí∞ 12 meses gratis | ‚úÖ S√≠ | ‚úÖ S√≠ | ‚≠ê‚≠ê Media | Media-Alta |
| **Replit / GitHub Codespaces** | üíµ Freemium | ‚ö†Ô∏è Limitado | ‚ö†Ô∏è Limitado | ‚≠ê‚≠ê‚≠ê F√°cil | F√°cil |
| **Docker Local** | üÜì Gratis | ‚úÖ S√≠ | ‚úÖ S√≠ | ‚≠ê Dif√≠cil | Media |

**Recomendaci√≥n:** **Google Colab + Databricks Community** (100% gratis, f√°cil integraci√≥n)

---

## ü•á OPCI√ìN 1: Google Colab (RECOMENDADO - 100% GRATIS)

### ‚úÖ Ventajas
- **100% GRATIS** (con GPU/TPU disponibles)
- PySpark pre-instalado
- Notebooks interactivos
- F√°cil integraci√≥n con tu plataforma web
- Sin l√≠mite de tiempo (mientras est√©s activo)
- Almacenamiento en Google Drive

### ‚ö†Ô∏è Limitaciones
- Airflow limitado (solo para demostraciones)
- Sesi√≥n se desconecta despu√©s de inactividad
- Recursos compartidos

### üöÄ Implementaci√≥n

#### 1. Crear Notebook para Ejercicios PySpark

Cada ejercicio PySpark puede tener su propio Colab notebook:

**Estructura:**
```
https://colab.research.google.com/drive/.../pyspark_exercise_001.ipynb
```

**Contenido del notebook:**
```python
# Instalar PySpark en Colab
!pip install pyspark

from pyspark.sql import SparkSession

# Crear sesi√≥n Spark
spark = SparkSession.builder \
    .appName("Exercise_001") \
    .getOrCreate()

# === EJERCICIO ===
# Estudiante completa aqu√≠
```

#### 2. Integrar con tu Plataforma Web

**Opci√≥n A: Enlaces directos** (M√°s simple)

```html
<!-- En courses.html o learn.html -->
<div class="pyspark-exercise">
    <h3>PySpark - Crear DataFrame con Schema</h3>
    <p>‚ö†Ô∏è Este ejercicio requiere entorno cloud</p>

    <a href="https://colab.research.google.com/drive/1abc..."
       target="_blank"
       class="btn-colab">
        <img src="https://colab.research.google.com/assets/colab-badge.svg">
        Abrir en Google Colab
    </a>
</div>
```

**Opci√≥n B: Iframe embebido** (M√°s integrado)

```html
<iframe
    src="https://colab.research.google.com/drive/1abc...?embedded=true"
    width="100%"
    height="600px"
    frameborder="0">
</iframe>
```

**Opci√≥n C: API de Colab** (M√°s avanzado)

```javascript
// En learn.js, detectar ejercicio PySpark
if (currentExercise.id.startsWith('pyspark_')) {
    // Mostrar bot√≥n "Abrir en Colab"
    const colabUrl = currentExercise.colabNotebookUrl;
    window.open(colabUrl, '_blank');
}
```

#### 3. Actualizar `learn_exercises.json`

```json
{
  "id": "learn_030",
  "title": "PySpark - Crear DataFrame con Schema",
  "instruction": "Define un schema expl√≠cito con StructType...",
  "theory": "...",
  "colabNotebookUrl": "https://colab.research.google.com/drive/1abc...",
  "requiresCloud": true,
  "cloudProvider": "Google Colab"
}
```

#### 4. Modificar `learn.js` para PySpark

```javascript
// En loadExercise()
if (currentExercise.requiresCloud) {
    displayCloudOption(currentExercise);
}

function displayCloudOption(exercise) {
    const container = document.getElementById('code-editor-container');

    container.innerHTML = `
        <div class="cloud-exercise-notice">
            <h3>‚òÅÔ∏è Ejercicio Cloud</h3>
            <p>Este ejercicio se ejecuta en Google Colab (gratis)</p>

            <a href="${exercise.colabNotebookUrl}"
               target="_blank"
               class="btn-open-colab">
                <img src="https://colab.research.google.com/assets/colab-badge.svg">
                Abrir Ejercicio en Google Colab
            </a>

            <div class="cloud-instructions">
                <h4>Instrucciones:</h4>
                <ol>
                    <li>Haz clic en el bot√≥n de arriba</li>
                    <li>Se abrir√° Google Colab en nueva pesta√±a</li>
                    <li>Completa el ejercicio siguiendo las instrucciones</li>
                    <li>Ejecuta todas las celdas</li>
                    <li>Verifica los resultados</li>
                </ol>
            </div>
        </div>
    `;
}
```

### üìù Creaci√≥n de Notebooks

**Script para generar notebooks autom√°ticamente:**

```python
# generate_colab_notebooks.py
import json
import nbformat as nbf

def create_pyspark_notebook(exercise):
    nb = nbf.v4.new_notebook()

    # Celda 1: Setup
    nb.cells.append(nbf.v4.new_code_cell("""
# Instalar PySpark
!pip install -q pyspark

from pyspark.sql import SparkSession
from pyspark.sql.types import *

# Crear sesi√≥n Spark
spark = SparkSession.builder \\
    .appName('{exercise_id}') \\
    .getOrCreate()

print('‚úì PySpark listo')
    """.format(exercise_id=exercise['id'])))

    # Celda 2: Teor√≠a
    nb.cells.append(nbf.v4.new_markdown_cell(f"""
## {exercise['title']}

### üìö Teor√≠a
{exercise['theory']}

### üìñ Sintaxis
```python
{exercise['syntax']}
```

### üí° Ejemplo
```python
{exercise['example']['code']}
```
    """))

    # Celda 3: Ejercicio
    nb.cells.append(nbf.v4.new_code_cell(f"""
# === TU C√ìDIGO AQU√ç ===
# {exercise['instruction']}

{exercise['starterCode']}
    """))

    # Celda 4: Validaci√≥n
    nb.cells.append(nbf.v4.new_code_cell(f"""
# === VALIDACI√ìN ===
try:
    {exercise['test']}
    print('‚úÖ ¬°Soluci√≥n correcta!')
except Exception as e:
    print(f'‚ùå Error: {{e}}')
    """))

    # Guardar
    filename = f"notebooks/{exercise['id']}.ipynb"
    with open(filename, 'w') as f:
        nbf.write(nb, f)

    print(f"‚úì Created {filename}")

# Cargar ejercicios
with open('ejercicios/learn_exercises.json') as f:
    data = json.load(f)

# Generar notebooks para PySpark
for ex in data['exercises']:
    if 'PySpark' in ex['title']:
        create_pyspark_notebook(ex)
```

### üîÑ Workflow del Usuario

1. Usuario abre ejercicio PySpark en tu web
2. Ve bot√≥n "Abrir en Google Colab"
3. Click ‚Üí abre Colab en nueva pesta√±a
4. Completa ejercicio en Colab
5. Ejecuta y valida resultados
6. (Opcional) Vuelve a tu plataforma y marca como completado

---

## ü•à OPCI√ìN 2: Databricks Community Edition (100% GRATIS)

### ‚úÖ Ventajas
- **100% GRATIS** permanentemente
- Cluster Spark completo (15GB RAM, 6GB storage)
- Entorno profesional de Data Engineering
- Notebooks colaborativos
- Cat√°logo de datos integrado
- Ideal para proyectos reales

### ‚ö†Ô∏è Limitaciones
- No tiene Airflow nativo
- Cluster se apaga despu√©s de 2 horas de inactividad
- 1 cluster a la vez
- Recursos limitados vs versi√≥n paga

### üöÄ Implementaci√≥n

#### 1. Registrarse
```
https://community.cloud.databricks.com/login.html
```

#### 2. Crear Workspace con Ejercicios

**Estructura de carpetas:**
```
Workspace/
‚îú‚îÄ‚îÄ Curso_PySpark/
‚îÇ   ‚îú‚îÄ‚îÄ 01_Basics/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Exercise_030_Schema.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Exercise_031_Select.py
‚îÇ   ‚îú‚îÄ‚îÄ 02_Advanced/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Exercise_040_Repartition.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Exercise_041_Broadcast.py
```

#### 3. Crear Notebooks Interactivos

**Ejemplo: Exercise_030_Schema**

```python
# Databricks notebook source
# MAGIC %md
# MAGIC # PySpark - Crear DataFrame con Schema
# MAGIC
# MAGIC ## üìö Teor√≠a
# MAGIC Definir schemas expl√≠citos mejora performance y validaci√≥n de datos.
# MAGIC
# MAGIC ## üéØ Objetivo
# MAGIC Define un schema con StructType y crea un DataFrame.

# COMMAND ----------

from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# === TU C√ìDIGO AQU√ç ===
# Define el schema para: nombre (string), edad (int), ciudad (string)

schema = ___

# Datos de ejemplo
data = [
    ("Ana", 25, "Lima"),
    ("Bob", 30, "Cusco"),
    ("Carlos", 28, "Arequipa")
]

# Crea el DataFrame
df = ___

# Muestra el resultado
df.show()
df.printSchema()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‚úÖ Soluci√≥n

# COMMAND ----------

# Descomentar para ver soluci√≥n
# schema = StructType([
#     StructField("nombre", StringType(), True),
#     StructField("edad", IntegerType(), True),
#     StructField("ciudad", StringType(), True)
# ])
#
# df = spark.createDataFrame(data, schema)

# COMMAND ----------

# MAGIC %md
# MAGIC ## üí° Pista 1
# MAGIC Usa StructType([StructField(...), ...])

# COMMAND ----------

# MAGIC %md
# MAGIC ## üí° Pista 2
# MAGIC spark.createDataFrame(data, schema)
```

#### 4. Compartir Notebooks

**Opci√≥n A: URL p√∫blica**
```
https://databricks-prod-cloudfront.cloud.databricks.com/public/.../exercise_030.html
```

**Opci√≥n B: Import desde GitHub**
```python
# Usuarios pueden importar tus notebooks directamente
# File ‚Üí Import ‚Üí URL
https://github.com/tu-repo/databricks-exercises/exercise_030.py
```

#### 5. Integrar con tu Plataforma

```javascript
// En learn.js
if (currentExercise.id.startsWith('learn_03') ||
    currentExercise.id.startsWith('learn_04')) {

    displayDatabricksOption(currentExercise);
}

function displayDatabricksOption(exercise) {
    const container = document.getElementById('code-editor-container');

    container.innerHTML = `
        <div class="databricks-exercise">
            <h3>‚òÅÔ∏è Databricks Community Edition</h3>
            <p>Este ejercicio usa un cluster Spark real (gratis)</p>

            <div class="steps">
                <h4>Primero (solo una vez):</h4>
                <ol>
                    <li>Crea cuenta gratuita en
                        <a href="https://community.cloud.databricks.com" target="_blank">
                            Databricks Community
                        </a>
                    </li>
                    <li>Inicia tu cluster (bot√≥n "Start Cluster")</li>
                </ol>

                <h4>Luego, para cada ejercicio:</h4>
                <ol>
                    <li>Importa el notebook desde GitHub</li>
                    <li>Completa el c√≥digo</li>
                    <li>Ejecuta las celdas</li>
                </ol>
            </div>

            <a href="${exercise.databricksNotebookUrl}"
               class="btn-databricks"
               target="_blank">
                üìä Abrir Notebook en Databricks
            </a>

            <a href="https://github.com/${exercise.githubNotebookPath}"
               class="btn-github"
               target="_blank">
                <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" width="20">
                Ver en GitHub
            </a>
        </div>
    `;
}
```

---

## ü•â OPCI√ìN 3: Google Cloud Platform (GCP) - $300 Cr√©ditos Gratis

### ‚úÖ Ventajas
- **$300 USD en cr√©ditos** para 90 d√≠as
- Dataproc (Spark managed)
- Cloud Composer (Airflow managed)
- Producci√≥n-ready
- Escalable

### üí∞ Costos despu√©s del free tier
- **Dataproc**: ~$0.01/hour (cluster peque√±o)
- **Cloud Composer**: ~$150/mes (environment peque√±o)
- **Compute Engine**: ~$5/mes (VM f1-micro)

### üöÄ Implementaci√≥n (Soluci√≥n Econ√≥mica)

#### Arquitectura Recomendada

```
Tu Plataforma Web (GitHub Pages - Gratis)
    ‚Üì
API Backend (Cloud Functions - Casi gratis)
    ‚Üì
Dataproc On-Demand (Solo cuando se ejecutan ejercicios)
```

**Costo estimado:** < $10/mes con uso moderado

#### 1. Cloud Functions como Backend

```javascript
// functions/execute-pyspark/index.js
const { Dataproc } = require('@google-cloud/dataproc');

exports.executePySpark = async (req, res) => {
    const { exerciseId, code } = req.body;

    // Crear cluster temporal
    const dataproc = new Dataproc.v1.ClusterControllerClient();

    const cluster = await dataproc.createCluster({
        projectId: 'tu-proyecto',
        region: 'us-central1',
        cluster: {
            clusterName: `exercise-${exerciseId}-${Date.now()}`,
            config: {
                masterConfig: {
                    numInstances: 1,
                    machineTypeUri: 'n1-standard-1'
                },
                workerConfig: {
                    numInstances: 2,
                    machineTypeUri: 'n1-standard-1'
                }
            }
        }
    });

    // Ejecutar c√≥digo
    const job = await dataproc.submitJob({
        projectId: 'tu-proyecto',
        region: 'us-central1',
        job: {
            placement: { clusterName: cluster.clusterName },
            pysparkJob: {
                mainPythonFileUri: `gs://tu-bucket/exercises/${exerciseId}.py`,
                args: [code]
            }
        }
    });

    // Esperar resultados
    const result = await job.promise();

    // Eliminar cluster (importante para no gastar)
    await dataproc.deleteCluster({
        projectId: 'tu-proyecto',
        region: 'us-central1',
        clusterName: cluster.clusterName
    });

    res.json({ success: true, output: result.output });
};
```

#### 2. Integrar con tu Web

```javascript
// En learn.js
async function executePySparkInCloud(code) {
    showLoadingModal('Ejecutando en Google Cloud...');

    try {
        const response = await fetch('https://us-central1-tu-proyecto.cloudfunctions.net/executePySpark', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                exerciseId: currentExercise.id,
                code: code
            })
        });

        const result = await response.json();
        hideLoadingModal();

        displayConsoleOutput(result.output, 'success');
    } catch (error) {
        hideLoadingModal();
        displayConsoleOutput(`Error: ${error.message}`, 'error');
    }
}
```

#### 3. Costos Optimizados

**Configuraci√≥n econ√≥mica:**
```yaml
# cluster-config.yaml
masterConfig:
  numInstances: 1
  machineTypeUri: e2-micro  # M√°s barato
  diskConfig:
    bootDiskSizeGb: 30

workerConfig:
  numInstances: 0  # Sin workers para ejercicios b√°sicos

softwareConfig:
  imageVersion: 2.0-debian10  # Latest stable

# Auto-delete despu√©s de inactividad
lifecycleConfig:
  idleDeleteTtl: 600s  # 10 minutos
```

**Costo estimado:**
- Cluster e2-micro: $0.006/hora
- 10 ejercicios/d√≠a √ó 5 min cada uno = 50 min/d√≠a
- 50 min/d√≠a √ó 30 d√≠as = 1500 min/mes = 25 horas/mes
- **Costo total: ~$0.15/mes** üéâ

---

## üèÖ OPCI√ìN 4: Replit (Freemium - F√°cil pero Limitado)

### ‚úÖ Ventajas
- F√°cil de usar
- IDE en el navegador
- Colaboraci√≥n en tiempo real
- Deployment simple

### ‚ö†Ô∏è Limitaciones
- Plan gratis muy limitado (500MB RAM)
- PySpark dif√≠cil de ejecutar
- Airflow no soportado

### üöÄ Implementaci√≥n

Solo para ejercicios Python ligeros, no recomendado para PySpark/Airflow.

---

## üêã OPCI√ìN 5: Docker Local (√öltimo Recurso)

### Cuando usar:
- Sin internet confiable
- M√°xima privacidad
- Desarrollo/testing local

### Setup Completo

```yaml
# docker-compose.yml
version: '3'
services:
  # PySpark Jupyter
  pyspark:
    image: jupyter/pyspark-notebook:latest
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
    environment:
      - JUPYTER_ENABLE_LAB=yes

  # Airflow
  airflow:
    image: apache/airflow:2.7.1
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
```

**Ejecutar:**
```bash
docker-compose up -d
```

---

## üéØ RECOMENDACI√ìN FINAL

### Para tu caso (Plataforma de Aprendizaje):

**Combinaci√≥n √ìPTIMA (100% Gratis):**

1. **Para PySpark:**
   - **Databricks Community** (ejercicios avanzados)
   - **Google Colab** (ejercicios b√°sicos)

2. **Para Airflow:**
   - **Google Colab** con Airflow standalone (demostraciones)
   - **Astronomer.io Free Tier** (30 d√≠as gratis, luego $10/mes)

3. **Para SQL:**
   - Ya resuelto ‚úÖ (SQL.js en navegador)

4. **Para Python:**
   - Ya resuelto ‚úÖ (Pyodide en navegador)

### Implementaci√≥n Sugerida

```json
// En learn_exercises.json
{
  "exercises": [
    {
      "id": "learn_030",
      "title": "PySpark - Schema",
      "cloudProvider": "databricks",
      "notebookUrl": "https://databricks.com/.../exercise_030",
      "alternativeUrl": "https://colab.research.google.com/.../exercise_030"
    },
    {
      "id": "learn_051",
      "title": "Airflow - DAG Basics",
      "cloudProvider": "colab",
      "colabUrl": "https://colab.research.google.com/.../airflow_dag"
    }
  ]
}
```

---

## üìã Plan de Acci√≥n

### Fase 1: Immediate (Esta semana)
1. ‚úÖ Crear cuenta Databricks Community
2. ‚úÖ Crear notebooks para ejercicios PySpark
3. ‚úÖ Integrar botones "Abrir en Databricks" en tu web

### Fase 2: Short-term (Pr√≥ximas 2 semanas)
1. Crear notebooks Colab como alternativa
2. Agregar ejemplos de Airflow en Colab
3. Documentar workflow para usuarios

### Fase 3: Long-term (Opcional)
1. Si creces: migrar a GCP con Cloud Functions
2. Monetizar plataforma para cubrir costos
3. Ofrecer certificados por curso completo

---

## üí° Extras: Monetizaci√≥n Futura

Si quieres eventualmente monetizar:

**Modelo Freemium:**
- **Gratis:** Python, SQL, notebooks est√°ticos
- **Pro ($5/mes):**
  - Ejecuci√≥n PySpark en cloud dedicado
  - Airflow projects ilimitados
  - Certificados verificables
  - Soporte prioritario

**Costo de operaci√≥n Pro:**
- GCP Cloud Functions: $0.40/month
- Dataproc on-demand: ~$2/month
- Total: < $3/month por usuario Pro
- **Margen: $2/usuario**

---

## üîó Enlaces √ötiles

- **Databricks Community:** https://community.cloud.databricks.com
- **Google Colab:** https://colab.research.google.com
- **GCP Free Tier:** https://cloud.google.com/free
- **Astronomer Free Trial:** https://www.astronomer.io/try-astro

---

**√öltima actualizaci√≥n:** 2025-11-02
